import sys
import threading
import numpy as np
import sounddevice as sd
from adafilt import MultiChannelBlockLMS
from vispy import app, scene
from vispy.scene.visuals import Text
from vispy.scene.widgets import Grid, ViewBox

# Parameters for real-time processing
# Query the available sound devices and print the information
print(sd.query_devices())

# Set input and output device indices (microphones and speaker)
input_device = 6  # Input device (microphones)
output_device = 6  # Output device (speaker via headphone jack)

# Get information about the selected input device
device = sd.query_devices(input_device)
print(device)

# Get the sample rate from the input device (sampling rate for audio processing)
samplerate = device["default_samplerate"]
print(samplerate)

# Set latency for the audio stream (low latency for real-time processing)
latency = "low"

# Set blocksize, the number of samples to process in one iteration (affects real-time performance)
blocksize = 2048  # Size of the audio block for processing (e.g., number of samples)

# Data type for the audio processing (32-bit floating point)
dtype = "float32"

# Define the number of channels for input (2 channels: reference and error microphones) and output (1 channel: speaker)
channels = inch, outch = (2, 1)  # 2 input channels (reference and error), 1 output channel (speaker)

# Adaptive filter (LMS)
# MultiChannelBlockLMS is used here, which is an adaptive filter that processes blocks of data.
filt = MultiChannelBlockLMS(
    Nin=2,  # Number of input channels (reference and error signals)
    length=blocksize * 4,  # Length of the adaptive filter (number of taps)
    blocklength=blocksize,  # Block size to process at a time
    leakage=0.99999999,  # Leakage factor for stability in filtering
    stepsize=0.1,  # Stepsize for adapting the filter
    constrained=True,  # Apply constraints to ensure stability
)

# Initialize arrays for error signal (e) and output signal (y)
e = np.zeros((blocksize, 1))  # Error signal (difference between desired and actual)
y = np.zeros((blocksize, 1))  # Output signal (anti-noise generated by filter)

# Initialize the secondary path model, initially set to zero
h_sec = np.zeros(blocksize * 4)  # Model of the secondary path (for filtering the reference signal)

# Define a Filter Monitor for visualization
class FilterMonitor:
    def __init__(self):
        # Create arrays to store the filter weights for plotting
        N = blocksize * 4  # Filter length
        self.pos1 = np.zeros((N, 2))  # Positions for plotting filter weights
        self.pos2 = np.zeros((N, 2))
        self.pos1[:, 0] = np.arange(N) / samplerate  # Time axis
        self.pos2[:, 0] = np.arange(N) / samplerate  # Time axis

        # Colors for the filter weights plots
        self.color1 = (1, 0, 0)  # Red for filter weight 1
        self.color2 = (0, 1, 0)  # Green for filter weight 2

        # Canvas setup for visualizing the filter weights
        canvas = scene.SceneCanvas(keys="interactive", show=True)
        main_grid = canvas.central_widget.add_grid()

        grid = Grid(border_color="r")  # Grid layout for the plot
        info = scene.widgets.ViewBox(border_color="b")  # Viewbox for additional information
        info.camera = "panzoom"  # Set the camera type
        info.camera.rect = -1, -1, 2, 2  # Set initial view area
        info.stretch = (1, 0.1)  # Stretch factor for layout

        main_grid.add_widget(grid, row=0, col=0)
        main_grid.add_widget(info, row=1, col=0)

        # Add axis widgets for plotting
        x_axis = scene.AxisWidget(orientation="bottom")  # Bottom axis for time
        x_axis.stretch = (1, 0.1)

        y_axis = scene.AxisWidget(orientation="left")  # Left axis for filter weights
        y_axis.stretch = (0.1, 1)

        grid.add_widget(x_axis, row=1, col=1)
        grid.add_widget(y_axis, row=0, col=0)

        # Create viewbox for plotting the filter weights
        viewbox = grid.add_view(row=0, col=1, camera="panzoom")
        x_axis.link_view(viewbox)
        y_axis.link_view(viewbox)

        # Add CPU load information display
        text = Text(text="TEXT", color=(1, 1, 1, 1), parent=info.scene)
        text.font_size = 18
        self.text = text

        # Line plot for the filter weights
        self.line1 = scene.Line(self.pos1, self.color1, parent=viewbox.scene)
        self.line2 = scene.Line(self.pos2, self.color2, parent=viewbox.scene)

        # Auto-scale view
        viewbox.camera.set_range()

    # Update method to refresh the visual representation of filter weights and CPU load
    def update(self, w, load):
        self.pos1[:, 1] = w[:, 0, 0]  # Update first filter weight plot
        self.pos2[:, 1] = w[:, 0, 1]  # Update second filter weight plot
        self.line1.set_data(pos=self.pos1, color=self.color1)
        self.line2.set_data(pos=self.pos2, color=self.color2)
        self.text.text = f"CPU load: {load * 100:.1f}%"  # Update CPU load display

# Initialize the FilterMonitor instance for visualization
filter_monitor = FilterMonitor()

# Callback function for audio stream (real-time audio processing)
def callback(indata, outdata, frames, time, status):
    global e, y, h_sec

    if status:
        print("Callback status:", status)

    try:
        # Reference signal from microphone 1 (input channel 1)
        x = indata[:, 0:1]  # Reference microphone (index 0)

        # Error signal from microphone 2 (input channel 2)
        d = indata[:, 1:2]  # Error microphone (index 1)

        # Generate the output signal (anti-noise) through the adaptive filter
        # Filter the reference signal using the secondary path model
        fx = np.convolve(x[:, 0], h_sec, mode="same")  # Filtered reference signal

        # Use the adaptive filter to predict the output signal (anti-noise)
        y[:] = filt.filt(fx[:, None])

        # Calculate the error signal (difference between desired signal and actual output)
        e[:] = d - y

        # Adapt the filter weights based on the error signal
        filt.adapt(fx[:, None], e)

        # Send the anti-noise signal to the speaker (output channel)
        outdata[:] = y

        # Update the filter visualization (show filter weights and CPU load)
        filter_monitor.update(filt.w, stream.cpu_load)

    except StopIteration:
        raise sd.CallbackAbort  # If iteration stops, abort the callback
    except Exception as e:
        print(type(e).__name__ + ": " + str(e))  # Handle any other exceptions
        raise sd.CallbackAbort  # Abort the callback on error

# Event for thread synchronization (ensure stream has finished processing)
callback_finished_event = threading.Event()

# Stream setup for real-time audio processing (using the selected input/output devices)
stream = sd.Stream(
    device=(input_device, output_device),  # Set input and output devices
    samplerate=samplerate,  # Set the sample rate
    blocksize=blocksize,  # Set the block size (number of samples per block)
    dtype=dtype,  # Set the data type (32-bit float)
    latency=latency,  # Set the latency for real-time processing
    channels=channels,  # Set the number of input and output channels
    callback=callback,  # Set the callback function for processing audio
    finished_callback=callback_finished_event.set,  # Event when the stream finishes
)

try:
    # Run the audio stream and start the real-time FxLMS process
    with stream:
        app.run()  # Run the Vispy app for visualization
except KeyboardInterrupt:
    sys.exit(0)  # Exit gracefully on keyboard interrupt
 plt.plot(filt.w)
plt.show()
